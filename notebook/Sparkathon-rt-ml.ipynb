{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkathon \n",
    "\n",
    "## Real time machine learning\n",
    "\n",
    "### 14.02.2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "\n",
    "1. Trochę teorii o przetwarzaniu strumieni w Sparku\n",
    "2. Dwa różnych API\n",
    "3. Estymatory dostępne w Sparku w trybie real time\n",
    "4. Przykład pracującego w trybie Real Time Estymatora\n",
    "6. Serving vs training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARK (STRUCTURED) STREAMING\n",
    "### Trochę teorii o przetwarzaniu strumieni w Sparku\n",
    "\n",
    "##### Batch vs Streaming\n",
    "\n",
    "Przy przekształcaniu batch'owym dla nas dostępny \"komplet\" danych do obróbki.\n",
    "\n",
    "Głowna idea w przekształceniu strumieniowym w tym żeby traktować dane jak tabełę do którę są dodawane ciągle są dodawane nowe linii.\n",
    "\n",
    "\n",
    "Jakie są różnicy pomiędzy tymi trybami przekształeń danych:\n",
    "\n",
    "**Stream**\n",
    "\n",
    "1. Robi przekształcenie albo na jednym elemncie albo na kilku elementach (na oknie) który sytem dostał\n",
    "2. Przekształcenia zazwyczaj proste (ze wzgłędu na potrzebę szybkiej odpowiedzi)\n",
    "3. Obliczenia zazyczaj niezależne (w kontekscie okna lub rekordu)\n",
    "4. Asynchroniczne zazwyczaj źródło danych nie interesuje się u nas czy otrzymaliśmy dane :)\n",
    "\n",
    "**Batch**\n",
    "\n",
    "1. Ma dostęp do wszystkich danych\n",
    "2. Może sobie pozwolić na przekształcenia o dowolnym skomplikowaniu, Bo:\n",
    "3. Job może trwać minuty (i godziny ;)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dwa różnych API dla Streaming'u i Dwa różnych API dla Machine Learning'u\n",
    "\n",
    "Spark ma bogatą historię wsparcia przetwarzania strumieniowego\n",
    "\n",
    "W 2012 do Spark'u dołączył projekt SPark Streaming ze Swoim **DStreams** API.\n",
    "To było jedno z pierwszych API (przynajmniej tak wyszło z mojego researchu :) którę dawało developerom możliwość \n",
    "użycia funkcji wysokopoziomowych, jakich jak _map_ i _reduce_.\n",
    "\n",
    "Dotychczas jest Spark Streaming jest używany w wielu organizacji (patrząc na ilość pytań na StackOverflow).\n",
    "\n",
    "Jednak _DStreams_ jak i RDD opierają się na obiektach Java/Python co w praktyce zawęża pole możliwych optymalizacji).\n",
    "\n",
    "\n",
    "W 2016 więc powstał project Structured Streaming który używa DataFrame'y i Datasety. Structured Streaming API jest oznaczone jako stabilne w wersji Spark'a 2.2.0 więc jest gotowy do komercyjnego użycia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARK MACHINE LEARNING (ML/MLlib)\n",
    "### Klasa ***Pipeline*** i jej działanie\n",
    "\n",
    "_Pipeline_ używa *Dataframe*'a jako źrodła danych. Dataframe może mieć różne dane: tekst, wertory, labeli wektorów wejściowych, predykcję, etc.\n",
    "\n",
    "_Pipeline_ to jest klasa którą reprezentuję sekwencję przekstałceń danych albo _Transformer_ albo _Estimator_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Pipeline ma metodę _fit()_ którą dopasowywuje ten Pipeline do danych. Po wykonaniu Metody _fit()_ Pipeline się dopasowuje do danych i wyniku otrzymujemy _PipelineModel_\n",
    "\n",
    "Kiedy metoda _fit()_ jest wołana:\n",
    "    * Dla każdego _Transformer'a_ w pipelinie wykonuje się metoda transform na tym Dataframe.\n",
    "    * Dla _Estimator'ów_ wykonuje się metoda _fit()_ która przekształca ten Estymator w Transformer. Jeżeli Pipeline ma więcej niż jeden Estymator, to na nie ostatnich też jest wykonywana _transform()_\n",
    "    \n",
    "    \n",
    "#### Przykład Pipeline\n",
    "przed wywołaniem metody _fit()_:\n",
    "\n",
    "![przed wykonaniem fit()](https://spark.apache.org/docs/latest/img/ml-Pipeline.png)\n",
    "\n",
    "1. Tokenizer — dzieli tekst na słowe\n",
    "2. HashingTF — konwertuje słowa na wektory\n",
    "3. Logistic regression — klasyfikuje wektor wejściowy\n",
    "\n",
    "\n",
    "po wykonaniu metody _fit()_:\n",
    "![po wykonaniu fit()](https://spark.apache.org/docs/latest/img/ml-PipelineModel.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Przykładowy Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasy i pojęcia używane w Machine Learning'u w Spark'u\n",
    "\n",
    "* Estimator\n",
    "    * Predictor (coś co daję predykcję, czyli zwraca klasę lub prawdopodobieństwo dla wertora wejściowego)\n",
    "        * Classifier\n",
    "            * Probabilistic\n",
    "            * OneVsRest\n",
    "            * ...\n",
    "        * Regressor\n",
    "            * LinearRegressor\n",
    "            * GeneralizedLR\n",
    "        * Random Forest/Tree\n",
    "    * **ALS model** (omówiliśmy na poprzednim meetupie, materiały dostępne są [tu](https://github.com/addmeaning/sparkathon-als)\n",
    "    * Pipeline (jedno lub więcej przekstałceń, albo Estimator albo Transformer)\n",
    "    * MinMaxScaler, LDA, etc\n",
    "* Transformer (przekstałca DataFrame na DataFrame (zazwyczaj wzbogacony o kolumnę)\n",
    "    * Model (= algoritm + dane)\n",
    "        * PipelineModel (fitted model)\n",
    "    * One-Hot encoder, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "// Prepare training documents from a list of (id, text, label) tuples.\n",
    "val training = spark.createDataFrame(Seq(\n",
    "  (0L, \"a b c d e spark\", 1.0),\n",
    "  (1L, \"b d\", 0.0),\n",
    "  (2L, \"spark f g h\", 1.0),\n",
    "  (3L, \"hadoop mapreduce\", 0.0)\n",
    ")).toDF(\"id\", \"text\", \"label\")\n",
    "\n",
    "// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "val tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "\n",
    "val hashingTF = new HashingTF().setNumFeatures(1000).setInputCol(tokenizer.getOutputCol).setOutputCol(\"features\")\n",
    "\n",
    "val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.001)\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))\n",
    "\n",
    "// Fit the pipeline to training documents.\n",
    "val model = pipeline.fit(training.asInstanceOf[Dataset[_]])\n",
    "\n",
    "// Now we can optionally save the fitted pipeline to disk\n",
    "model.write.overwrite().save(\"/tmp/spark-logistic-regression-model\")\n",
    "\n",
    "// We can also save this unfit pipeline to disk\n",
    "pipeline.write.overwrite().save(\"/tmp/unfit-lr-model\")\n",
    "\n",
    "// And load it back in during production\n",
    "val sameModel = PipelineModel.load(\"/tmp/spark-logistic-regression-model\")\n",
    "\n",
    "// Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "val test = spark.createDataFrame(Seq((4L, \"spark i j k\"),\n",
    "  (5L, \"l m n\"),\n",
    "  (6L, \"spark hadoop spark\"),\n",
    "  (7L, \"apache hadoop\")\n",
    ")).toDF(\"id\", \"text\")\n",
    "\n",
    "// Make predictions on test documents.\n",
    "model.transform(test.asInstanceOf[Dataset[_]]).select(\"id\", \"text\", \"probability\", \"prediction\").collect().foreach {\n",
    "    case Row(id: Long, text: String, prob: Vector, prediction: Double) =>\n",
    "        println(s\"($id, $text) --> prob=$prob, prediction=$prediction\")\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estymatory w Sparku dostępne w trybie Real Time\n",
    "\n",
    "Używając API Dataframe/Pipeline wszystkie Estymatory którę dobrzę się skladają w Pipeline (czyli te, których dodanie kolumny z predykcją następuje po wywołaniu metody _transform()_ mogą służyć dla tego żeby używać ich w czasie rzeczywistym :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Czas na przykład\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/70/Solid_white.svg/768px-Solid_white.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving vs training\n",
    "Na poprzednim przykładzie widać że algorytm może być stosowany dla danych streaming'owych natomiast nie jest update'owany na bierząco.\n",
    "\n",
    "Na drugim przykładzie pokaże mock systemu, który mogł pomóc w rozwiązaniu tego problemu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ograniczenia\n",
    "\n",
    "API cały czas się ewoluuje i jak widzicie nie wszystkie możliwości są dostępnę (szczególnie w DataFrame/Structured Streaming API).\n",
    "\n",
    "Natomiast są też w starym API Spark'u algorytmy którę mogą updejtować swoje koeficienty.\n",
    "\n",
    "To są: (TODO)\n",
    "    * First\n",
    "    * Second\n",
    "    * Third\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A jak u konkurencji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Flink: biblioteka ml działa wyłącznie dla dla DataSet API (czyli batch). Nie można konwertować DataStream na DataSet. Też zapłanowany development \n",
    "\n",
    "* Amazon Machine learning (co prawda to serwis a nie framework) nie ma retraining'u\n",
    "\n",
    "* Oryx 2 (To w sumie Spark) używa rozwiązania która opiera się na logice 2 przykładu (ale oczywiście tam jest wszystko poprawnie rozbudowano)\n",
    "\n",
    "* Mahout ma Streaming Kmeans (spark też ma)\n",
    "\n",
    "* Tensorlow no online retraining\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
